# Google File System

### 1. Introduction

1）组件错误经常发生：持续的检测、错误检测、容错、自动恢复

2）文件非常大：重新设计I\O操作和磁盘块的大小

3）大多数文件操作是被添加而不是覆盖：几乎不用随机写，末尾添加数据操作的性能优化和原子性保证非常重要

4）良好的接口设计提高整个系统灵活性

### 2. 设计概览

#### 2.1 一些假设

1）系统建立于并不按昂贵的商用机器上，容易出错

2）系统存储大量大文件（每个大概100MB或更大），GB级别的文件也很常见，需要高效管理

3）系统执行大量顺序读，很少的随机读

4）系统执行大量尾部添加的写操作，后续几乎不会修改

5）系统要实现良好的接口，便于多个客户并行写文件

6）追求高吞吐量，对低时延没有要求

#### 2.2 接口

GFS提供常见的IO操作接口，还提供snapshot和record两种append方式。

#### 2.3 架构

- 一个master + 多个chunkserver，可以被多个client访问
- 文件被分为固定大小的chunk，为每个chunk创建不可变的唯一chunk handle进行标识
- master维护文件系统的所有元数据，管理系统级别的活动，周期性地与chunkserver通信
- GFS client代表应用与master和chunkserver进行交互
- client和chunkserver都不会缓存文件数据，所以没有缓存一致性问题（操作系统的buffer cache照样会缓存）

#### 2.4 master

单一的master节点可以使系统的管理变得简单，但要注意不要让master涉及过多的读写操作，否则单一master很容易成为瓶颈。client会向master询问应该去哪一个chunkserver读取数据，并会缓存这个信息以在此后的一连串操作都使用这个信息。

#### 2.5 Chunk大小

每个chunk被涉及为64MB，这远比传动的文件系统block要大。每个chunk副本只在需要时才会扩展，避免浪费空间。存储碎片是大chunk设计方案最大的弊端。

优点
- 减少了client和master交互的开销，因为一次就可以读足够多的数据
- client一次读完整个chunk后就可以断开与chunkserver的TCP连接，减少维持连接的开销
- 减少了mster上元数据的体积

即使减少存储空间的申请避免碎片，依然有一些缺点：
- 存有小文件的chunkserver可能会成为热点（一种解决方式是增加副本数量，以及错开client访问时间）

#### 2.6 元数据

三种主要的元数据
- 文件和chunk的命名空间
- 文件到chunk的映射关系
- 每个chunk副本的位置
他们均存储在master的内存中，其中命名空间和映射关系还另外存储在master的磁盘上，同时备份在一些远程机器上。

##### 2.6.1 内存中的数据结构

存储在内存中扫描更快，master以此来进行chunk垃圾回收，chunkserver失败时的副本备份、迁移副本以平衡负载和磁盘空间。

一个潜在的缺点是总量会被限制，于是chunk的数量收到限制，但在实践中不成问题。如果需要支持更大的文件系统，添加额外内存的代价相对收益来说也问题不大。

##### 2.6.2 chunk位置

master轮询（监听？）chunk的位置，不做持久化存储。一开始是持久化的，但后来改了方案。

###### 2.6.3 操作日志

既是元数据的持久化，也保留了并发操作执行的时间线。

#### 2.7 一致性模型

##### 2.7.1 GFS的保证

文件命名空间的变更是原子的。

##### 2.7.2 对应用的影响



