# 4. GFS

可拓展的分布式文件系统。



## 4.1 设计要点

- 节点失效将被看成是正常情况，而不再视为异常情况，整个系统包含几百个节点
- 按照传统标准来看，文件都是非常巨大的。数个GB的文件是常事
- 大部分文件都是只会在文件尾新增加数据，而少见修改已有数据的
- 与应用一起设计的的文件系统API对于增加整个系统的弹性适用性有很大的好处

**约定：**

- 系统是建立在大量廉价的普通计算机上，这些计算机经常故
- 系统存储了大量的超大文件。同时必须支持小型文件，但是我们不必为小型文件进行特别的优化。
- 一般的工作都是由两类读取组成：大的流式读取和小规模的随机读取。在大的流式读取中，每个读操作通常要读取几百k的数据，每次读取1M或者以上的数据也很常见。对于同一个客户端来说，往往会发起连续的读取操作顺序读取一个文件。小规模的随机读取通常在文件的不同位置，读取几k数据。
- 通常基于GFS的操作都有很多超大的，顺序写入的文件操作。一旦完成写入，文件就很少会更改。对于文件的随机小规模写入是要被支持的，但是不需要为此作特别的优化。
- 系统必须非常有效的，明确细节的对多客户端并行添加同一个文件进行支持。我们的文件经常使用生产者/消费者队列模式，或者作为多路合并模式进行操作。

- 高性能的稳定带宽的网络要比低延时更加重要。



## 4.2 架构

![](https://markdownimg-1255784639.cos.ap-shanghai.myqcloud.com/cpp_network/gfs%E6%9E%B6%E6%9E%84.png)

单个的 **master**和好多个 **chunkserver**（块服务器）组成。

文件被划分为 固定大小chunk, 每个chunk被全局唯一的64位 chunk handle标记，chunk文件在chunkserver本地以linux文件的形式存储。每个chunk被复制保存在多个chunk server上。

**主节点：** 

- 保存文件系统元数据：命名空间，访问控制信息，文件到chunk的映射，每个chunk的位置。 
- 管理系统级的活动：chunk分配、孤儿chunk的GC、chunk在chunkserver间的迁移。 
- 主节点通过心跳包发送指令和收集从节点状态。

- **客户端和master进行元数据操作，所有数据相关通信直接与chunkserver交互**

## 4.3 Single Master

**为了不让主节点成为瓶颈，客户端的读写操作不经过主节点。**  客户端把应用要读取的文件名和偏移量，根据固定的chunk大小，转换成文的chunk index，然后向master发送这个包含了文件名和chunk index的请求，master返回chunk handle和chunk 位置，chunk server与客户端交互。在一个chunk内，客户端不用再与master通信。客户端通常都会在请求中附加向master询问多个chunk的信息，master于是接着会立刻给这个客户端回应这些chunk的信息。

## 4.4 chunk size

选择一个很大的chunk :

1.  减少客户端与master的交互
2.  客户端在一个chunk上完成更多的操作，可以维护一个客户端和chunk server的TCP长连接来减少网络流量
3. 减少元数据的大小，利于将元数据保存在内存中

**问题：** 个可执行的程序在GFS上保存成为一个单chunk的文件，并且在数百台机器上一起启动的时候就出现焦点问题。可以增加文件保存的份数，一劳永逸的解决方法是让客户端能够互相读取数据，这样才是解决之道。

## 4.5 metadata

master节点保存三个主要的元数据：文件和chunk 的namespace、文件到chunks的映射关系、每个chunk的副本的位置。保存在内存中，不持久化保存chunk信息，重启后访问每个chunk获取信息。

- in-memory data strucytures
- chunk locations
- operation log : 保存关键的元数据变化历史记录，

## 4.6 一致性

## 4.7 系统交互

### 4.7.1 Leases and Mutation Order

mutation  : 改变chunk内容或者chunk的原数据的操作，比如改写或者增加操作。
